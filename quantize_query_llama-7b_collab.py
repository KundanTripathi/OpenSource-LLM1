# -*- coding: utf-8 -*-
"""quantize_phi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yJDwWQkGWQjevdI67R_gO4vw0fO8-K7O
"""

from google.colab import drive
drive.mount('/content/drive')

project_path = '/content/drive/MyDrive/llm_quantization'

"""
env_content = """ 'Your HF Token' """

env_path = f"{project_path}/.env"

with open(env_path, "w") as f:
    f.write(env_content)

print(f".env file created at {env_path}")
"""

!pip install python-dotenv

# Load .env from project path
import os
from dotenv import load_dotenv

env_path = os.path.join(project_path, ".env")

load_dotenv(env_path)

# Access your keys
hf_token = os.getenv("HF_TOKEN")
"""
if hf_token:
    print("HF_TOKEN:", hf_token)
else:
    print("HF_TOKEN not found in .env")
"""

import torch
print(torch.cuda.is_available())  # Should print: True
print(torch.cuda.get_device_name(0))  # Should print: Tesla T4

!pip install -q transformers bitsandbytes accelerate huggingface_hub

#os.rmdir(os.path.join(project_path, 'llama2_7b_quantized'))

#for i in os.listdir(os.path.join(project_path, 'llama2_7b_quantized')):
#  os.remove(os.path.join(project_path, 'llama2_7b_quantized',i))

#os.rmdir(os.path.join(project_path, 'llama2_7b_quantized'))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import login
from google.colab import drive
import os


output_path = os.path.join(project_path, 'llama2_7b_quantized')

#loging to HF
login(token=hf_token)

# Define model
model_name = "codellama/CodeLlama-7b-Instruct-hf"

#model_name = "NousResearch/Llama-2-7b-hf"

# Define 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # Normal float 4-bit
    bnb_4bit_compute_dtype=torch.float16,  # Compute in float16 for speed
    bnb_4bit_use_double_quant=True  # Nested quantization for better accuracy
)

# Load tokenizer
print(f"Loading tokenizer from {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load model with 4-bit quantization
print(f"Loading and quantizing model to 4-bit precision...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",  # Auto-map to GPU
    torch_dtype=torch.float16
)

# Save quantized model and tokenizer to Google Drive
print(f"Saving quantized model to {output_path}...")
model.save_pretrained(output_path, safe_serialization=True)
tokenizer.save_pretrained(output_path)
print(f"Quantized model and tokenizer saved to {output_path}")

# Verify model size
print(f"Checking saved model size...")
model_size = sum(os.path.getsize(os.path.join(output_path, f)) for f in os.listdir(output_path)) / (1024**3)
print(f"Model size: {model_size:.2f} GB")

quantized_path = os.path.join(project_path, 'llama2_7b_quantized')


# Load tokenizer and model from Google Drive
print(f"Loading quantized model from {quantized_path}...")
tokenizer = AutoTokenizer.from_pretrained(quantized_path)
model = AutoModelForCausalLM.from_pretrained(
    quantized_path,
    device_map="auto"
)

# Prepare input prompt
prompt = "What is the capital of India?"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# Perform inference
print("Generating response...")
outputs = model.generate(
    **inputs,
    max_length=50,
    num_return_sequences=1,
    do_sample=False
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print response
print(f"Prompt: {prompt}")
print(f"Response: {response}")

#######